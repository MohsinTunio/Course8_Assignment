---
title: "MachineLearning_Assignment"
author: "Mohsin"
date: "October 03, 2018"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# ---***Data Loading***---
### We are loading training and testing datasets from the working directory.
```{r, echo=TRUE}
training_data<-read.csv("pml-training.csv")
testing_data<-read.csv("pml-testing.csv")
```

# ---***Data Purging***---
### Here we are going to delete all columns that contain NA values and do away with the attributes not present in the testing data 
### The following attributes contain NA values: variance, mean and standard deviation. We are going to remove these variables.
### Furthermore, we are also going to do away with the first 7 columns that are having non-numeric nature. 

# ---***splitting the data***---
### We are going to split the dataset with 60% in the training set and the remaining 40% in the testing set. 

### In the end, we will filter out the characteristics that are in the testing.


```{r, echo=TRUE,comment=" "}
library(caret)
library(rpart)
library(rattle)
library(rpart.plot)
library(e1071)
library(ggplot2)
library(lattice)


attributes <- names(testing_data[,colSums(is.na(testing_data)) == 0])
attributes<-attributes[8:59]

training_data <- training_data[,c(attributes,"classe")]
testing_data <- testing_data[,c(attributes,"problem_id")]

dim(training_data) 
dim(testing_data)


set.seed(12345)

inTrain <- createDataPartition(training_data$classe, p=0.6, list=FALSE)
train <- training_data[inTrain,]
test <- training_data[-inTrain,]

dim(train)
dim(test)
```

# ---***Build  Decision Tree and Prediction Models***---
### In this part we shouldn't  be thinking of accuracy to be near 100% rather if it's near 80% it is worth accepting.
### Drecision Tree Model Prediction 
### Random Forest Model:An error  estimate of less than 3% should be expected.
### Random Forest Model Predictions
### Confusion Matrix 
### Predicting Through Decision Tree
### Predicting Through Random Forest-the optimal one

```{r, echo=TRUE,comment=" "}
library(randomForest)
library(RColorBrewer)


model_DTree <- rpart(classe ~ ., data = train, method="class")
fancyRpartPlot(model_DTree)


set.seed(12345)

predicting <- predict(model_DTree, test, type = "class")
confusionMatrix(predicting, test$classe)

 

set.seed(12345)
model_RF <- randomForest(classe ~ ., data = train, ntree = 1000)

predicting <- predict(model_RF, test, type = "class")

confusionMatrix(predicting, test$classe)

predicting_DTree <- predict(model_DTree, testing_data, type = "class")
predicting_DTree

predicting_RF <- predict(model_RF, testing_data, type = "class")
predicting_RF

```




